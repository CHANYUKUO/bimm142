---
title: "Lab7"
author: "Chan-yu Kuo"
date: "2023-01-31"
output: pdf_document
---

Explore clustering and dimensionality reduction

K means number of clusters

```{r}
## K-means

hist(rnorm(30,-3))
hist(rnorm(30,3))
## use to make random number in normal distribution
tmp<-c(rnorm(30,-3),rnorm(30,3))
x<-cbind(x=tmp,y=rev(tmp))
head(x) 
```

```{r}
plot(x)
## they are supposed by be +- 3 on both x and y, reread how x and y are set up if not sure. X= -3 ascending, y = 3 descending. 
```

```{r}
km<-kmeans(x,centers=2,nstart=10)
## k == 2, have two center, it will find the distance of any given point and classify into either of the two groups. nstart = if centers is a number, how many random sets should be chosen?
km


```

how many points in each cluster:

```{r}
km$size
```

component of your result object details cluster assignment: cluster vector cluster center: centers

```{r}
km$cluster
km$centers
```

plot x colored by the kmeans cluster assignment and add cluster centers as blue points

```{r}
plot(x,col=c("red","blue"))## raw data visualize
plot(x,col=km$cluster)## km$cluster gives vector 1 and 2, indicating different cluster, in vector, from top to bottom. 
points(km$centers,col="blue")## add points at the centers
```

```{r}
km<-kmeans(x,centers=4,nstart=20)
plot(x,col=km$cluster)
points(km$centers,col="blue")
```

Hierarchical Clustering This is another very useful and widely employed clustering method.

```{r}
d<-dist(x)## d is the distance, if having two points, the distance between the point will be calculated. 
hc<-hclust(d) 
hc
```

```{r}
plot(hc)## What is this height? 
```

```{r}
plot(hc)
abline(h=10,col="red")
```

Cut the tree to yield sub-tree. Put all the members within the tree into new membership

```{r}
h<-cutree(hc,h=10)
plot(x,col=h)
```

use k= to cutree rather than h= height of cutting with cutree. k= will give the number of cluster you want.

```{r}
newh<-cutree(hc,k=4)
newh
```

```{r}
## PCA
### principle component, new low dimensional axis closest to the observations
### we always have pc1, then depend on the needs, we can have pc2, pc3 etc. 
```

```{r}
#Q1 17 rows and 5 columns, include one name column
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
dim(x)
head(x)
```

```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

```{r}
##Q2
dim(x)
x <- read.csv(url, row.names=1) ## this helps reduce steps we need to assign names. This is faster and simpler. However, this might be difficult if the names are not stored in column but in rows. 
head(x)
```

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
## q3, change the beside from true to false will result in this plot
```

```{r}
str(x)
```

```{r}
pairs(x, col=rainbow(17), pch=16)
##Q5 x is the inputm col is the color of each point ( in rows), pch specify the color. 
## pairs essentially gather all the value of a row from the 4 column, and compare each of them by displaying one column value in x and another column value in y. 

## a diagnole line means the comparsion between two contry is small (they are similar)
## N Ireland have highest difference in cost , but can't tell the detail on what the color represent if we do not predefined the color ourselves. 
cbind(row.names(x),rainbow(17))

```

```{r}
pca <- prcomp( t(x) )
summary(pca)
## proportion of variance, 67.44 % of data point will be shown based on the PC. 
## Cumulative proportion: add up from the proportion of variation. 


```

```{r}
## PCA plot (a.k.a) Score plot PC1 VS PC2
 pca$x
```

```{r}
plot(pca$x[,1],pca$x[,2],col=c("orange","blue","red","green"))
## green represent N ireland, shows further away
```

z

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x),col=c("orange","red","blue","green"))
```

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
## or the second row here...
z <- summary(pca)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```
