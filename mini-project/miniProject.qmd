---
title: "hw8_miniproject"
format: pdf
editor: visual
---

## Preparing the data

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)
```

Diagnosis should contain the diagnosis column from wisc

```{r}
wisc.data <- wisc.df[,-1]
diagnosis <- wisc.df$diagnosis
head(diagnosis)
head(wisc.data)
head(wisc.df)
```

Q1 How many observations are in this dataset? 569 rows and 31 columns

```{r}
dim(wisc.df)
```

Q2 How many of the observations have a malignant diagnosis? 212

```{r}
nrow(wisc.df[diagnosis=="M",])
```

Q3How many variables/features in the data are suffixed with `_mean`? 10 based on reading the structure of the data

```{r}
str(wisc.df)
```

PCA components::

```{r}
# Check column means and standard deviations
## How to scale? What is appropriate?
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp( wisc.data, scale=TRUE)
## Scale is appropriate if the mean and sd varied by a lot between cases.
```

```{r}
summary(wisc.pr)
```

Q4: From the result above, PC1 cover 44.27% of variance

Q5: To get at least 70% of variance, we need PC1,PC2, and PC3

Q6: To get at least 90% of variance, we need at least PC 1,2,3,4,5,6,7.

Q7: What stands out to you about this plot? Nothing, I can barely understand anything as it is filled with a bunch of different information.

```{r}
biplot(wisc.pr)

```

```{r}
diagnosis
```

```{r}
```

Q8 The PCA plots shows some clustering distinct from one another (red and black). PC1 Vs PC2 looks better than PC1 VS PC3, as one of the black dot show up in the red region.

```{r}
head(wisc.pr$x)
```

`{plot( wisc.pr$x, col=factor(diagnosis),}      xlab = "PC1", ylab = "PC2")`

```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,3], col = factor(diagnosis), 
     xlab = "PC1", ylab = "PC3")
```

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=df$diagnosis) + 
  geom_point()
```

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

-   **Q9.** For the first principal component, what is the component of the loading vector (i.e.Â `wisc.pr$rotation[,1]`) for the feature `concave.points_mean`?

        -0.26085376  is the component for first PC, concave.point_means

```{r}
wisc.pr$rotation[,1]
```

Q10, We need at least PC5 to get 80% coverage

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist,method="complete")
plot(wisc.hclust)
abline(h=20, col="red", lty=2)
```

Q11: The height of 20

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 4)
table(wisc.hclust.clusters, diagnosis)
```

**Q12.** Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10? If cut at 10, cluster 1,2,5, yields better cluster for M cells whereas 4,7 yields better cluster for B cells

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 10)
table(wisc.hclust.clusters, diagnosis)
```

-   **Q13.** Which method gives your favorite results for the same `data.dist` dataset? Explain your reasoning. Ward.D2 explain better because it provide a better organization of the data. If you cut with only two cluster, the result it still usable ( good cluster) compare to the others

```{r}
plot(hclust(data.dist,method="single"))
plot(hclust(data.dist,method="complete"))
plot(hclust(data.dist,method="average"))
plot(hclust(data.dist,method="ward.D2"))
```

Combining Methods

```{r}
wisc.pr.hclust<- hclust(data.dist,method="ward.D2")
grps <- cutree(wisc.pr.hclust, k=2)
plot(wisc.pr$x[,1:2], col=grps)
plot(wisc.pr$x[,1:2], col=factor(diagnosis))
g <- as.factor(grps)
levels(g)
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

Q 15: the new model have 188/(188+28) of B cells in Cluster 1 and 329/(329+24) of M cells in Cluster 2.

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
# Scale the wisc.data data using the "scale()" function

wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
plot(wisc.pr.hclust)
table(wisc.pr.hclust.clusters,diagnosis)
```
